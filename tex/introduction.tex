\chapter{Introduction}

\section{Statement of Purpose}

Recognizing activities in videos has become a hot topic over the last years in the computer vision community~\cite{ngiam2011multimodal}. The exponential growth of portable video cameras and online multimedia repositories, as well as recent advances in video coding, storage and computational resources have motivated an intense research in the field towards new and more efficient solutions for organizing, understanding and retrieving video content.

Deep learning techniques have recently become the new state of the art in many computer vision tasks, such as image and object recognition in still images. While successful methodologies have been presented for image understanding, video content still presents additional challenges (e.g. motion, temporal consistency, ...)  that often cannot be bridged with still image recognition solutions.

The purpose of this work is to address the challenges of video content analysis taking advantage of state-of-the-art deep learning techniques. The aim of this project is to develop a competitive framework to both classify and temporally localize activities on videos. To achieve this goal, the dataset used to fulfill this task is be the ActivityNet dataset \cite{caba2015activitynet}, which offers untrimmed videos depicting a diversity of human activities.

%Rather than focusing on activity classification on videos, the aim of this project is to offer a good framework to detect and localize activities on videos. To achieve this goal, the dataset used to fulfill this task will be the ActivityNet dataset \cite{caba2015activitynet}, which offers untrimmed videos which a huge variety of activities on it.

% AMAIA: I think that it is fine to say that you will both address activity classification and detection in your thesis, not just detection... DONE

In particular, this project's main contributions are:
\begin{itemize}
	\item The design and training of a deep learning model architecture, which is based on 3D Convolutional features and Recurrent Neural Networks.
    \item The development and analysis of post processing techniques for classification and temporal localization
    \item The release of an open sourced package containing all the tools to reproduce the experiments, as well as the conversion of a state-of-the-art C3D model from Caffe to Keras.

% REMOVED:    \item A framework to classify and localize activities on videos using deep learning techniques, more precisely using recurrent neural networks.
%    \item Develop techniques to process sequence output from Recurrent Neural Networks, to get the activities' localization on videos.
%    \item Explore different configurations of Neural Networks to achieve the best results in activity classification and detection.
%    \item Contribute to the research community porting a model to extract features from video from one deep learning framework to another and open sourcing the code.
\end{itemize}

% AMAIA: I am not sure whether these are your contributions or a summary list of everything you did in the thesis. For contributions, I would stick to:
% 1) The design and training of the architecture, which is based on 3D conv features and RNNs
% 2) The development & analysis of postprocesssing techniques for classification & detection
% 3) The release of an opensourced package containing all the tools to reproduce the experiments, as well as the conversion of a state-of-the-art 3CD model from Caffe to Keras.

This project has been developed at the \textit{Image Processing Group (GPI) of the Universitat Polit√®cnica de Catalunya (UPC)} during the Spring 2016 semester. This group had already work in deep learning techniques for still images, but never before in video analytic. %In this sense, this is a pioneering work that sets a baseline technique and software for future research within the group.
%REMOVED: This project has no baseline and all the code and development of deep learning techniques applied to video has been done from scratch.
% AMAIA: What do you mean with this sentence? I guess this is assumed in all projects unless stated otherwise... ALBERTO: I tried to say that it has all been done from scratch with no previous work on this field on the Image group.

\section{Requirements and Specifications}

This project has been developed with the implicit goal of setting a baseline for video analytic with deep learning in the research group, so that future students and researchers to keep working on it. The requirements of this project are the following:
\begin{itemize}
    \item Design and train a deep neural network to classify and temporally localize activities on videos using the ActivityNet Dataset.
    \item Participate in the ActivityNet Challenge 2016, organized as a workshop in the top conference IEEE Conference on Computer Vision and Pattern Recognition (h5-index = 128).
\end{itemize}

% AMAIA: I guess that by requirements you mean "what you were asked to do/what we agreed to do" at the beginning of the project? I slightly modified it, uut feel free to add more items.

As this project has been developed from scratch, no prior specifications were defined. The specifications were decided taking into account the needs for the project and the available resources. All the development was done on \textit{Python} using a very well-known framework which is \textit{Keras}, also a pioneering use of this library in GPI. This Deep Learning wrapper facilitates the design and training of models over two computational frameworks: \textit{Theano}\cite{theano2016theano} and \textit{TensorFlow}\cite{abadi2016tensorflow}. Both  projects support complex and high demanding computations over both CPU and GPU. For this project \textit{Theano} was used as back-end because, at the time of developing this project, it was the only one that had implemented the convolution 3D and max pooling 3D operations required for its development.

In addition to the software, specific hardware was required. The high demanding computational resources needed to train neural networks required the use of GPUs provided by the \textit{Image Processing Group} at UPC.

%DELETED: For the development of Deep Learning models and to do all the computations were used some frameworks in \textit{Python}. Because this project has been developed without a baseline, no prior specifications were required. Anyway, for the development of this project it has been decided to work with \textit{Python} for development and \textit{Keras} over \textit{Theano}\cite{theano2016theano} as the deep learning framework to use.

% AMAIA: "Anyway" is too informal...
% AMAIA: I don't know if this applies here, but what about the GPUs that you used? ALBERTO: I've just added

\section{Methods and Procedures}

This project tries to offer a simple and flexible solution to face the tasks classification and temporal localization on videos. The solution proposed is made by a first step stage where the video features are extracted. To do so, a pre-trained 3D convolutional network has been used to extract features from spatial and temporal dimension from video frames. In parallel, features from audio were extracted to combine them with the ones extracted from the video frames.

As a second stage, after having encode all the possible information from video using available implementations, a Recurrent Neural Network was proposed to use to exploit long term relations and predict the sequence of activities available at each video. This network was tested with different architectures and inputs from the previous features extracted. Figure~\ref{fig:global_pipeline} shows an schematic of the proposed pipeline.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=0.8\linewidth]{img/introduction/schematic_pipeline}
\end{center}
\caption{Global architecture of the solution proposed on this project.}
\label{fig:global_pipeline}
\end{figure}

All the tests were done using a recently publish video dataset, the ActivityNet which offers untrimmed videos from 200 activities. All the videos from the dataset present a single activity on it so its possible to classify them globally, furthermore the activities along the video are temporally localized and so the task to do it was the main goal of this project. Most of the effort and architecture was done taking into account resolving the detection task knowing that if you solve the temporal detection, is possible to extrapolate to globally classify videos. 

Moreover, multiple post-processing techniques were proposed to use at the proposed network output to improve the classification and detection tasks. Multiple configurations were tested in order to maximize the results.

Finally, the best results were submitted to the ActivityNet Challenge 2016 at the CVPR.

\section{Work Plan}

This project was planned to follow the packages detailed on this section, with the exception of some minor deviations described in Section~\ref{section:work_plan_deviations}.

\subsection{Work Packages}

\begin{itemize}
    \item WP 1: Project Documentation
    \item WP 2: Research for the State of the Art
    \item WP 3: Dataset to work with
    \item WP 4: Software to use
    \item WP 5: Experimentation and Results Evaluation
    \item WP 6: ActivityNet Challenge 2016 Participation
    \item WP 7: Delivery and Exposition of this project
\end{itemize}

\subsection{Gantt Diagram}

The Gantt diagram of this project's work plan can be found on the Figure~\ref{fig:gantt_diagram}.

\begin{figure}[H]
\begin{center}
\includegraphics[width=1\linewidth]{img/introduction/gantt_diagram}
\end{center}
\caption{Gantt diagram of this thesis.}
\label{fig:gantt_diagram}
\end{figure}

%\textcolor{red}{XAVI: Do not forget your Gantt diagram. You can always check the meetings notes I took if you do not remember the exact timing of each task. This document should help you in reviewing everything you have been doing since February.}


\section{Work Plan Deviations and Incidents}
\label{section:work_plan_deviations}

The initial plan was to adapt the C3D\cite{tran2014learning} model for the ActivityNet task, which is implemented with the  \textit{Caffe} deep learning framework. But in order to use Recurrent Neural Networks, \textit{Keras} was a much better and flexible framework, so it was necessary to export the original C3D model from the Caffe framework to Keras.

Another deviation for the plan was that, due to the huge amount of required computational resources, fine tuning the full C3D network was not been possible. Instead, the C3D framework was used to extract features from the videos, which were later used as inputs to train the recurrent neural network.

Finally, the original work plan was extended by exploring the potential of audio descriptors for activity recognition. For this task, this work was supported by Professor Ignasi Esquerra from the TALP research group at UPC. He took care of extracting the audio features, which were later assessed and fused with the visual ones.
