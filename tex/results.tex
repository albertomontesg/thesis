\chapter{Results}
\label{section:results}

On this section, the results of the multiple experimentation will be given and detailed.

\section{Evaluation Metric}

In task like classification and detection, the research community and also the ActivityNet Challenge use the same metrics for evaluation to be able to compare results between publications. For the classification tasks, for each instance more than one prediction can be made, and for each prediction giving a the probability of success computed. With this multiple predictions for each instance some metric are computed. The simplest one is the \textit{Hit@k}, which gives you the proportion of the instance where the ground truth is on the \textit{top-k} predictions given. For the ActivityNet Challenge $k$ was set to 3.

In addition to this metric, for classifications tasks, it is mostly use the mean Average Precision (mAP) metric. It is computed as the mean of the average precision of all $M$ classes at the Dataset.

\begin{equation}
	mAP = \frac{1}{M} \sum_{m=1}^{M} AP(m)
\end{equation}

At the same time, the Average Precision for each class is computed as the average of the precision at position $n$, showed as $P(n)$. This precision is defined as the amount of elements in $k_m$ between positions 1 and $n$ in the ranked list divided by $n$.

\begin{equation}
	AP(m) = \frac{1}{k_m} \sum_{n=1}^{k_m} P(n)
\end{equation}

For the other task of the Challenge, the temporal localization or detection as the Challenge name it, the metric use to verify the results given is the Intersection Over Union (IoU). This metric compute a prediction as correct if the IoU of the prediction with the ground truth annotations is higher than a value $\alpha$. Then the mAP is computed. For the ActivityNet Challenge the $\alpha$ value used is 0.5.

For all the metrics computations of this section, it has been used the evaluation scripts given by the ActivityNet Challenge 2016 organization.

\section{Training}

The training of the Recurrent Neural Networks was all done using a 256 batch size and 20 \textit{timesteps} to get a good gradient propagation through the \textit{timesteps} and to fit the most possible data into the GPUs when training.

The optimizer function use was the RMSprop\cite{dauphin2015rmsprop} which is known to work very well for training Recurrent Neural Networks. The RMSprop was set with a value of $\rho$ of 0.9 and $\epsilon$ of $10^{-8}$. The value of the learning rate, as it is explained on Section~\ref{section:training}, was set to $10^{-5}$ for all the experimentation done. 

% In order to try to fit the most possible data into the GPUs when training each batch and to get a good gradient propagation along the batch's sequence length, for all the experiments done on this project, the batch size was 256 and the timesteps 20.

\section{Classification Task}

The first experiments made were related to the Recurrent Neural Network architecture, in order to obtain the best performance. The way to measure the performance, was computing the mAP for the classification task. The different architectures first tested were related to the deep of the network. On the first experiment, it was tested from a 3 layers LSTM with 1024 cells each layer, to a one single layer with 512 cells, going through a two layers network with 512 LSTM cells each one. All the architectures presented a batch normalization after the input entrance and dropout before and after the LSTM layers. Also all the experiments were done exclusively with the features extracted from the C3D network.

As is showed on Table~\ref{table:classification_by_architecture}, the network that achieved the best performance was achieved with the simplest one as was expected while training due to the fact that all the networks presented high learning capacity over our data.

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Architecture & mAP & Hit@3 \\
\hline\hline
3 x 1024-LSTM & 0.5635 & 0.7437 \\
2 x 512-LSTM & 0.5492 & 0.7364 \\
1 x 512-LSTM & \bf0.5938 & \bf0.7576 \\
\hline
\end{tabular}
\end{center}
\caption{Results for classification task comparing different deep architectures. All values with
         only video features on the validation dataset.}
\label{table:classification_by_architecture}
\end{table}

Once the best architecture was chosen in the sense of deepness, the network was trained comparing the input features used and also comparing the basic architecture with the one that feedback from the previous output. On the Table~\ref{table:classification_by_features} can be seen how the basic architecture and with only features from the video obtain the best results.

It can be due to the fact that training the feedback architecture with the ground truth instances of the previous output may cause instabilities\cite{}.% ALBERTO: Bibliography?
On the other hand, doing some exploration over the videos and its audio tracks, it has been observed that in many cases, the audio was completely unrelated, mostly of having music over any kind of activity happening on the video. 

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Features used & mAP & Hit@3 \\
\hline\hline
Only video & \bf0.5938 & \bf0.7576 \\
Video w/ audio & 0.5755 & 0.7352 \\
Only video \& feedback & 0.5210 & 0.6982 \\
Video w/ audio \& feedback & 0.5652 & 0.7319 \\
\hline
\end{tabular}
\end{center}
\caption{Results for classification task with the model made by one 512-LSTM. Compare between
         features and feedback on the validation dataset.}
\label{table:classification_by_features}
\end{table}

The ActivityNet Dataset offers a 200 activities videos, but all this activities come from a taxonomy with hierarchically activities. The 200 activities which are annotated all the videos of the dataset and the proposed network predict are the leaf nodes of the taxonomy tree. On this structure there is 5 top level categories which all the videos belongs and describe more generally the activity happening on the video. These are: \textit{Eating and drinking activities}, \textit{Sports, Exercise and Recreation}, \textit{Household Activities}, \textit{Socializing, Relaxing and Leisure}, \textit{Personal Care}.

For all these top level activities, the mean average precision has been computed. To do so, all the predicted and ground truth activity labels have been changed by its top level activity and then for each of them computed the Average Precision. As can be seen on Table~\ref{table:top_level_classification_ap} and Figure~\ref{table:top_level_classification_ap} the \textit{Sports, Exercise and Recreation} have an average precision of $94\%$. This can be explained with the fact that all the features extracted from the videos come from a network which weights were trained to work with a dataset of videos, the Sports1M\cite{KarpathyCVPR14}.

\begin{table}[H]
\begin{center}
\begin{tabular}{|r|c|}
\hline
\textbf{Global Activities} & \textbf{AP} \\
\hline\hline
Eating and drinking Activities & 0.56942 \\
Sports, Exercise, and Recreation & 0.93662 \\
Household Activities & 0.74177 \\
Socializing, Relaxing, and Leisure & 0.76494 \\
Personal Care & 0.59931 \\
\hline\hline
\textbf{Global} (mAP) & 0.72241 \\
\textbf{Global} (Hit@3) & 0.92703 \\
\hline
\end{tabular}
\end{center}
\caption{Mean Average Precision computed for the top level activities of the ActivityNet Dataset. The results are computed over the validation dataset.}
\label{table:top_level_classification_ap}
\end{table}

\begin{figure}[H]
\begin{center}
\includegraphics[width=1\linewidth]{img/results/top_activities_classification_ap}
\end{center}
\caption{Representation of the results over the top-level activities}
\label{fig:top_level_classification_ap}
\end{figure}


 %%% Better giving this figure or the one with all the activities and see that mAP varies a lot from one activity to another?
\begin{figure}[H]
\begin{center}
\includegraphics[width=1\linewidth]{img/results/high_low_map_classification}
\end{center}
\caption{Sample of activities with the highest and lowest mAP for the classification task}
\label{fig:map_by_activity_classification}
\end{figure}

As an extra for visualizing the results on the classification task, at the Figure~\ref{fig:confussion_matrix} on the Appendix it is attached the confusion matrix of the top-1 activity prediction with the ground truth.

At the time to submit the solution proposal to the ActivityNet Challenge 2016, it was chosen the basic architecture with only the video features as input to make the predictions. The results in both validation and test subset submitted are displayed on Table~\ref{table:classification_results_challenge}.

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Subset & mAP & Hit@3 \\
\hline\hline
Validation & 0.5938 & \bf0.7576 \\
Test & 0.5874 & 0.7554 \\
\hline
\end{tabular}
\end{center}
\caption{Results submited to the ActivityNet Challenge 2016 for the classification task.}
\label{table:classification_results_challenge}
\end{table}



\section{Detection Task}

As the main goal of this project is to obtain a good network to temporally localize activities on videos, it has been compared the results on the detection task of the ActivityNet Challenge for the two architectures proposed. For the basic architecture the network was feed up with video features from the C3D network, while on the feedback architecture, the audio features were concatenated at the input as in classification task improve the results (check Table~\ref{table:classification_by_features}).

%%%%%%%%
How I justify the feedback architecture does not give best results than the basic one?
%%%%%%%%

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|}
\hline
Architecture & mAP \\
\hline\hline
Basic Architecture & \bf0.22513 \\
Feedback Architecture & 0.20676 \\
\hline
\end{tabular}
\end{center}
\caption{Best results obtain for the temporal activity localization task in the two architectures}
\label{table:detection_architecture_comparison}
\end{table}

As shows the Table~\ref{table:detection_architecture_comparison} the basic architecture get a slightly better results. The computation of the mean average precision was done doing the same post-processing to the predicted output of the network proposed on this project.

As it was explained on Section~\ref{section:post_processing}, a post-processing was done to the output to achieve a smoother and better temporal prediction of the activities. During this project there were performed some experiments to maximize the prediction precision for different values of $\gamma$ as the activity probability threshold and $k$ as smoothing factor for the mean filter. For different combinations of these variables it was performed the Table~\ref{table:detection_postprocessing_comparison}.

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
$\gamma$ & $k=0$ & $k=5$ & $k=10$ \\
\hline
0.2 & 0.20732 & \bf0.22513 & 0.22136 \\
0.3 & 0.19854 & 0.22077 & 0.22100 \\
0.5 & 0.19035 & 0.21937 & 0.21302 \\
\hline
\end{tabular}
\end{center}
\caption{mAP with an IOU threshold of $0.5$ over validation dataset. Here there is a comparison
between values of $k$ and $\gamma$ on post processing.}
\label{table:detection_postprocessing_comparison}
\end{table}

As can be seen, the best performance was achieved with an activity threshold $\gamma=0.2$ and smoothing filter of $k=5$. The effect of the both of the operations performed after the prediction can be seen on Figures~\ref{fig:smoothing_effect} and~\ref{fig:activty_threshold_effect}. On both figures it is displayed temporally the location of the activity at the ground truth of the video and also the prediction done with the proposed network before and after of each of the post-processing.

\begin{figure}[ht]
\begin{center}
%\includegraphics[width=1\linewidth]{img/results/smoothing_effect_1}
%\includegraphics[width=1\linewidth]{img/results/smoothing_effect_2}
%\includegraphics[width=1\linewidth]{img/results/smoothing_effect_3}
\includegraphics[width=1\linewidth]{img/results/smoothing_effect_4}
\includegraphics[width=1\linewidth]{img/results/smoothing_effect_5}
\includegraphics[width=1\linewidth]{img/results/smoothing_effect_6}
\end{center}
\caption{Effect of the mean filter with $k=5$ achieving a smoother activity prediction.}
\label{fig:smoothing_effect}
\end{figure}

\begin{figure}[ht]
\begin{center}
\includegraphics[width=1\linewidth]{img/results/activity_threshold_effect_1}
\includegraphics[width=1\linewidth]{img/results/activity_threshold_effect_2}
\includegraphics[width=1\linewidth]{img/results/activity_threshold_effect_3}
\end{center}
\caption{Activity Localization using different values of the threshold $\gamma$}
\label{fig:activty_threshold_effect}
\end{figure}

In addition, as it has been done for the classification task, the Average Precision has been computed for the top level activities of the ActivityNet Dataset taxonomy. As can be seen on Table~\ref{table:top_level_detection_ap} and Figure~\ref{fig:top_level_detection_ap} all the top level activities present a similar precision in activity temporal localization except from the category \textit{Personal Care} which does not achieve the half of precision of the rest of top level categories. Also remark that the top level category with highest precision is \textit{Sports, Exercise and Recreation} as happens on the classification task.

\begin{table}[H]
\begin{center}
\begin{tabular}{|r|c|}
\hline
\textbf{Global Activities} & \textbf{AP} \\
\hline\hline
Eating and drinking Activities & 0.25582 \\
Sports, Exercise, and Recreation & 0.30023 \\
Household Activities & 0.26252 \\
Socializing, Relaxing, and Leisure & 0.26060 \\
Personal Care & 0.11234 \\
\hline\hline
\textbf{Global} (mAP) & 0.23830 \\
\hline
\end{tabular}
\end{center}
\caption{Average Precision of activity localization computed for the top level activities of the ActivityNet Dataset. The results are computed over the validation dataset and with a IoU threshold of 0.5.}
\label{table:top_level_detection_ap}
\end{table}

\begin{figure}[H]
\begin{center}
\includegraphics[width=1\linewidth]{img/results/top_activities_detection_ap}
\end{center}
\caption{Representation of the results over the top-level activities}
\label{fig:top_level_detection_ap}
\end{figure}


%%%%%%%%
I will put a figure plotting the mAP against the IoU asked between 0.1 and 0.5
%%%%%%%%

Plot with multiple IOU and mAP... And maybe a plot of recall vs IoU threshold to see if the network can create good temporal candidates. 

\section{Results Visualization}

To check the behavior of the proposed model beyond the results of the metrics, at Figure~\ref{fig:results_visualization_classification} there are plot some examples of results in classification task, showing the top 3 activities predicted for the each video. At the same time, at Figure~\ref{fig:results_visualization_detection} there is plot the activity prediction done by the proposed network.

Even though the challenge required to temporally localize one single activity per video, the proposed network offers more flexibility at the time of prediction. It is capable to predict more than one activity for a video taking the maximum probable activity class after the smoothing filter. With this process, the prediction is a sequence of activities that can be process into multiple temporal activities localization in a single video. On Figure~\ref{fig:results_visualization_detection_classes} can be seen how would be the prediction. In some cases can be seen how more than one activity class is predicted and would require more exploration over the dataset to check if the predictions may have any sense.

\begin{figure}[H]
\begin{center}
%%%% Still have to do it
%\includegraphics[width=1\linewidth]{img/results/results_visualization_classification}
\end{center}
\caption{Results for the classification task}
\label{fig:results_visualization_classification}
\end{figure}

%%% I'll put two more examples to fill the page on all this 3 figures.
\begin{figure}[H]
\begin{center}
\texttt{Video ID: p1gH8y8X0kA}
\includegraphics[width=1\linewidth]{img/results/results_visualization_detection_a_1}
\includegraphics[width=1\linewidth]{img/results/results_visualization_detection_a_2}
\texttt{Video ID: AFb77tjPuwQ}
\includegraphics[width=1\linewidth]{img/results/results_visualization_detection_b_1}
\includegraphics[width=1\linewidth]{img/results/results_visualization_detection_b_2}
\texttt{Video ID: w--X02F3MHM}
\includegraphics[width=1\linewidth]{img/results/results_visualization_detection_c_1}
\includegraphics[width=1\linewidth]{img/results/results_visualization_detection_c_2}
\texttt{Video ID: cXY-ONmtylc}
\includegraphics[width=1\linewidth]{img/results/results_visualization_detection_d_1}
\includegraphics[width=1\linewidth]{img/results/results_visualization_detection_d_2}
\end{center}
\caption{Activity prediction done by the proposed neural network.}
\label{fig:results_visualization_detection}
\end{figure}

\begin{figure}[H]
\begin{center}
\texttt{Video ID: En9FemmDusk}
\includegraphics[width=1\linewidth]{img/results/results_visualization_detection_classes_a_1}
\includegraphics[width=1\linewidth]{img/results/results_visualization_detection_classes_a_2}
\texttt{Video ID: 6Ke30NtYOC0}
\includegraphics[width=1\linewidth]{img/results/results_visualization_detection_classes_b_1}
\includegraphics[width=1\linewidth]{img/results/results_visualization_detection_classes_b_2}
\texttt{Video ID: 32H1n87WgCM}
\includegraphics[width=1\linewidth]{img/results/results_visualization_detection_classes_c_1}
\includegraphics[width=1\linewidth]{img/results/results_visualization_detection_classes_c_2}
\texttt{Video ID: feWO\_gqAcGk}
\includegraphics[width=1\linewidth]{img/results/results_visualization_detection_classes_c_1}
\includegraphics[width=1\linewidth]{img/results/results_visualization_detection_classes_c_2}
\end{center}
\caption{Activity predicted from video doing non maximum suppression at each video clip. Different colors represents different activity classes.}
\label{fig:results_visualization_detection_classes}
\end{figure}


