\chapter{Results}
\label{section:results}

On this section, the results of the multiple experimentation will be given and detailed.

\section{Evaluation Metric}

In task like classification and detection, the research community and also the ActivityNet Challenge use the same metrics for evaluation to be able to compare results between publications. For the classification tasks, for each instance more than one prediction can be made, and for each prediction giving a the probability of success computed. With this multiple predictions for each instance some metric are computed. The simplest one is the \textit{Hit@k}, which gives you the proportion of the instance where the ground truth is on the \textit{top-k} predictions given. For the ActivityNet Challenge $k$ was set to 3.

In addition to this metric, for classifications tasks, it is mostly use the mean Average Precision (mAP) metric. It is computed as the mean of the average precision of all $M$ classes at the Dataset.

\begin{equation}
	mAP = \frac{1}{M} \sum_{m=1}^{M} AP(m)
\end{equation}

At the same time, the Average Precision for each class is computed as the average of the precision at position $n$, showed as $P(n)$. This precision is defined as the amount of elements in $k_m$ between positions 1 and $n$ in the ranked list divided by $n$.

\begin{equation}
	AP(m) = \frac{1}{k_m} \sum_{n=1}^{k_m} P(n)
\end{equation}

For the other task of the Challenge, the temporal localization or detection as the Challenge name it, the metric use to verify the results given is the Intersection Over Union (IoU). This metric compute a prediction as correct if the IoU of the prediction with the ground truth annotations is higher than a value $\alpha$. Then the mAP is computed. For the ActivityNet Challenge the $\alpha$ value used is 0.5.

For all the metrics computations of this section, it has been used the evaluation scripts given by the ActivityNet Challenge 2016 organization.

\section{Classification Task}

The first experiments made were related to the Recurrent Neural Network architecture, in order to obtain the best performance. The way to measure the performance, was computing the mAP for the classification task. The different architectures first tested were related to the deep of the network. On the first experiment, it was tested from a 3 layers LSTM with 1024 cells each layer, to a one single layer with 512 cells, going through a two layers network with 512 LSTM cells each one. All the architectures presented a batch normalization after the input entrance and dropout before and after the LSTM layers. Also all the experiments were done exclusively with the features extracted from the C3D network.

As is showed on Table~\ref{table:classification_by_architecture}, the network that achieved the best performance was achieved with the simplest one as was expected while training due to the fact that all the networks presented high learning capacity over our data.

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Architecture & mAP & Hit@3 \\
\hline\hline
3 x 1024-LSTM & 0.5635 & 0.7437 \\
2 x 512-LSTM & 0.5492 & 0.7364 \\
1 x 512-LSTM & \bf0.5938 & \bf0.7576 \\
\hline
\end{tabular}
\end{center}
\caption{Results for classification task comparing different deep architectures. All values with
         only video features on the validation dataset.}
\label{table:classification_by_architecture}
\end{table}

Once the best architecture was chosen in the sense of deepness, the network was trained comparing the input features used and also comparing the basic architecture with the one that feedback from the previous output. On the Table~\ref{table:classification_by_features} can be seen how the basic architecture and with only features from the video obtain the best results.

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Features used & mAP & Hit@3 \\
\hline\hline
Only video & \bf0.5938 & \bf0.7576 \\
Video w/ audio & 0.5755 & 0.7352 \\
Only video \& feedback & 0.5210 & 0.6982 \\
Video w/ audio \& feedback & 0.5652 & 0.7319 \\
\hline
\end{tabular}
\end{center}
\caption{Results for classification task with the model made by one 512-LSTM. Compare between
         features and feedback on the validation dataset.}
\label{table:classification_by_features}
\end{table}

The ActivityNet Dataset offers a 200 activities videos, but all this activities come from a taxonomy with hierarchically activities. The 200 activities which are annotated all the videos of the dataset and the proposed network predict are the leaf nodes of the taxonomy tree. On this structure there is 5 top level categories which all the videos belongs and describe more generally the activity happening on the video. These are: \textit{Eating and drinking activities}, \textit{Sports, Exercise and Recreation}, \textit{Household Activities}, \textit{Socializing, Relaxing and Leisure}, \textit{Personal Care}.

For all these top level activities, the mean average precision has been computed. To do so, all the predicted and ground truth activity labels have been changed by its top level activity and then for each of them computed the Average Precision. As can be seen on Table~\ref{table:top_level_ap} the \textit{Sports, Exercise and Recreation} have an average precision of $94\%$. This can be explained with the fact that all the features extracted from the videos come from a network which weights were trained to work with a dataset of videos, the Sports1M\cite{KarpathyCVPR14}.

\begin{table}[H]
\begin{center}
\begin{tabular}{|r|c|}
\hline
\textbf{Global Activities} & \textbf{AP} \\
\hline\hline
Eating and drinking Activities & 0.56942 \\
Sports, Exercise, and Recreation & 0.93662 \\
Household Activities & 0.74177 \\
Socializing, Relaxing, and Leisure & 0.76494 \\
Personal Care & 0.59931 \\
\hline\hline
\textbf{Global} (mAP) & 0.72241 \\
\textbf{Global} (Hit@3) & 0.92703 \\
\hline
\end{tabular}
\end{center}
\caption{Mean Average Precision computed for the top level activities of the ActivityNet Dataset. The results are computed over the validation dataset.}
\label{table:top_level_ap}
\end{table}

\begin{figure}[H]
\begin{center}
\includegraphics[width=1\linewidth]{img/results/top_activities_map_2}
\end{center}
\caption{Representation of the results over the top-level activities}
\label{fig:top_level_classification_map}
\end{figure}



\begin{figure}[H]
\begin{center}
\includegraphics[width=1\linewidth]{img/results/high_low_map_classification}
\end{center}
\caption{Sample of activities with the highest and lowest mAP for the classification task}
\label{fig:map_by_activity_classification}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=1\linewidth]{img/results/confussion_matrix}
\end{center}
\caption{Confusion matrix of the top-1 activity predicted with the ground truth}
\label{fig:confussion_matrix}
\end{figure}

At the time of submitting the predictions made with the best model for the testing dataset the mAP obtained was of $0.58741$ and the Hi@3 of $0.75548$.

\section{Detection Task}

The same here with some variations on the post-processing+


\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|}
\hline
Architecture & mAP \\
\hline\hline
Basic Architecture & \bf0.22513 \\
Feedback Architecture &  0.22377\\
\hline
\end{tabular}
\end{center}
\caption{Best results obtain for the temporal activity localization task in the two architectures}
\label{table:detection_architecture_comparison}
\end{table}


\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
$\gamma$ & $k=0$ & $k=5$ & $k=10$ \\
\hline
0.2 & 0.20732 & \bf0.22513 & 0.22136 \\
0.3 & 0.19854 & 0.22077 & 0.22100 \\
0.5 & 0.19035 & 0.21937 & 0.21302 \\
\hline
\end{tabular}
\end{center}
\caption{mAP with an IOU threshold of $0.5$ over validation dataset. Here there is a comparison
between values of $k$ and $\gamma$ on post processing.}
\label{table:detection_comparison}
\end{table}


\begin{figure}[H]
\begin{center}
%\includegraphics[width=1\linewidth]{img/results/confussion_matrix}
\end{center}
\caption{Activities with higher and lower mAP for the detection task}
\label{fig:map_by_activity_detection}
\end{figure}

Plot with multiple IOU and mAP...

\section{Results Visualization}

Here attach some figures about the predictions made by our model

