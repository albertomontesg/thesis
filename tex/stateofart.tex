\chapter{State of the Art}

Over the last years deep learning techniques have evolved and have achieved incredible results for impressive tasks.
%AMAIA: I feel the message you are trying to give in this sentence is too broad ...? I think you can jump straight ahead to talk about deep learning solutions for video.

% AMAIA: For the state of the art, I think that you should re-organize it to cover the main different challenges of video analysis, and explain what people do to tackle each (even though you end up citing the same paper more than once in the process). You should also explain what you do in your project, and how it resembles or differs from what others did.

% 1. How do people use information in different frames? Some use 2D convolutions and merge them somehow, others use 3D conv. Here you can talk about pretrained networks (some people use VGG imagenet, there's people who have trained other models for Sports1M, right?)
% 2. How is motion encoded ? Some works directly use optical flow, while others directly input the visual data.
% 3. How do people ensure temporal consistency? LSTMs or GRUs
% 4. Particularly for activity detection, how do people do this? I remember the paper from columbia, where there are several stages. Maybe a strength of your work is that you train a single network that does it all (you just postprocess the output).
% ...


Many works in the literature have explored activity and action recognition problems using deep learning strategies. Most of them can be classified by the deep learning techniques used. Most of them used an approach of two stages (encoder and decoder) to learn from video's dataset. The first stage, the one being consider a decoder, is the stage that tries to encode the visual and temporal information from the input videos into some features vectors or information. The second stage, also known as decoder, tries from the extracted encoded information from the first stage, make a prediction of the output, which can be a classification of the video or a temporal localization of an activity.

Is very common see that as encoder, use Convolutional Neural Networks (or also known as CNNs) as it has been widely demonstrated that applying CNN networks to images and videos has obtain very good results in classifications tasks. A very well known and used CNN network is the VGG\cite{Simonyan14c} network which was top scored on ImageNet Challenge in 2014 on classification task. This network uses 2D convolutional kernels to extract spatial correlations from images and therefor learn how to classify them. Applying this techniques to activity detection, there are some implementations\cite{simonyan2014two}\cite{yeung2015every}\cite{Ng_2015_CVPR}\cite{ballas2015delving} using mostly the VGG network to encode video information.

% AMAIA: I think it's a good idea to cover the cited works in more detail. What are their differences? What do they have in common? Do you take ideas from them? What are the differences between what they do and what you do?

On the other hand very recently was proposed a CNN which tries to exploit both spatial and temporal correlations in video data using the additional dimension that videos offer and images do not. This is the 3D convolutional network, also referenced as C3D\cite{tran2014learning}. This network uses 3D kernels rather than 2D to extract videos information and try to learn from them. It has been widely used\cite{baccouche2011sequential}\cite{tran2015deep}\cite{tran2014learning}\cite{shoutemporal} for applications such as video classification. On some other research papers\cite{Yao_2015_ICCV}\cite{zhang2016modelling} the both approaches have been tried using 2D and 3D convolutional networks.

For this network, the input data is very common to use the raw video and let the network learn and extract information. Feeding the network with all the pixels of each frame in bunches of frames (the C3D is fed up with a 16 frame video clip) is very common, but in other cases some tricks are done. To feed up 2D convolutional layers but trying to learn from temporal correlations, what is done is compute the optical vector between frames and give it to a 2D CNN for training in combination with a parallel CNN fed up with the raw frames\cite{Ng_2015_CVPR}\cite{Yao_2015_ICCV}.

% AMAIA: "Bunches", "fed up", too informal...

%%%% Comments
Now talk about the recurrent neural network architecture proposed in the different papers of the state of the art. Talk about what RNN and LSTM are, some equations?

% AMAIA: Don't use equations in the state of the art. Maybe later in the methodology

Talk about some architectures interesting in the activity detection field and explain them more detailed (not much detailed but a little)??

% AMAIA: Yes, definitely ! I think that what you will explain in later sections is primarily focused on detection, so it is relevant that you explain what people have done in the past, and how your approach differs from them. I think what is interesting in your work is that you don't train in several stages using segment candidates. You train a single network. Ok, it does not improve the results, but the approach is much nicer ideally. 
